\chapter{Módulo de Filtro}
\label{filtro_label} 

\section{Introducción}
La visión artificial es el proceso sensorial más complejo de todos.
Las tareas en visón por computador se pueden enumerar en:
\begin{enumerate}
\item Visión de bajo nivel. (Tareas automáticas)
  \begin{itemize}
  \item Captación. Obtención de la imagen.
  \item Preprocesamiento. Incluye técnicas como reducción de ruido y realce de detalles.
  \end{itemize} 
\item Visión de nivel medio. (Etiquetar objetos)
  \begin{itemize}
  \item  Segmentación. Localización de los objetos de interés.
  \item Descripción. Obtención de características: tamaño, formas, etc.
  \end{itemize} 
\item Visión de alto nivel. (Emular inteligencia)
  \begin{itemize}
  \item Reconocimiento. Identificación de objetos: tornillos, puertas, etc.
  \item Interpretación. Significado de un conjunto de objetos.
  \end{itemize} 
\end{enumerate} 

El objetivo de los filtros utilizados en nuestro proyecto entra en el ámbito del preprocesamiento y segmentación de imágenes.

Ejemplos de la fase de preprocesamiento son el suavizado, el realce, detección de bordes, detección de umbral, etc.

El procesamiento de una imagen puede ser visto como una transformación de una imagen en otra imagen, es decir, a partir de una imagen, se obtiene otra imagen modificada. Desde el punto de vista de visión artificial, el único propósito del procesamiento de imágenes es conseguir mas adelante un análisis de estas mas simple y mas fiable. Por consiguiente, el procesamiento de imágenes debe facilitar la extracción de información para un posterior análisis, de manera que la escena pueda ser interpretada de alguna manera.
\bigskip 

Por este motivo aplicamos a la imagen capturada por la webcam una serie de filtros, para simplificar la imagen, hasta el punto de eliminar la información que no nos interesa y realzar la información importante para el análisis posterior de la imagen, en este caso el reconocimiento de gestos y carteles.
\bigskip 

Para saber mas sobre imágenes digitales, en subsección \ref {imagen_digital} (página \pageref{imagen_digital}).
\bigskip 

La capacidad visual del robot, dependerá de los módulos de visión que estén activos. De momento solo hay módulos implementados y activos que permiten al robot recibir ordenes con gestos hechos con unos guantes o también recibir información procedente de carteles. Los carteles no solo pueden darle ordenes, si no hacerle una pregunta de la cual tenga conocimiento o hacerle realizar una operación aritmético-lógica.
\bigskip 

Por tanto los únicos medios para comunicarse con el robot son guantes y carteles especiales, de momento.

\bigskip 
Son especiales por su color. La razón de utilizar colores especiales ha sido crear en las imágenes capturadas, regiones de color con unos rangos de intensidades en los tres colores, mas separadas del resto de intensidades del histograma de la imagen. Así podremos aislar esta región, la del color especial. En el caso de los guantes, es la posición de los dedos la que indica la orden, es en los dedos donde esta el color especial, así que si solo nos quedamos con las regiones de este color y el resto lo despreciamos, estaremos simplificando muchísimo la imagen para un posterior análisis de esta. Lo mismo pasaría con los carteles, desechamos toda la imagen que no forme parte del cartel y dentro del cartel nos quedamos solo con la frase.

\bigskip
Los módulos posteriores a los filtros son módulos de análisis que deben de recibir la información lo mas clara posible, en el caso de los gestos se utiliza una red neuronal, la cual tiene que ser entrenada con imágenes muy simplificadas para que el entrenamiento tenga efecto y que las imágenes que reciba una vez entrenada, sean filtradas de la misma manera, para generar imágenes iguales que con las que fue entrenada, para poder reconocerlas. Respecto a los carteles el siguiente modulo es un OCR, muy sensible a ruidos, por tanto hay que asegurar que el filtro es efectivo, para que la salida de este no sea incoherente.

\section{Filtro de gestos}
Como ya hemos dicho este filtro sirve como preprocesamiento y segmentación de la imagen. Ya que el suavizado reduce los ruidos y la extracción de regiones de color localiza los objetos de interés, con el objetivo de pasarle una información mucho mas comprensible y simplificada a la red neuronal.
\bigskip 
Los gestos al robot se realizan con la ayuda de 2 guantes, uno para la mano izquierda que dará las ordenes y otro para la derecha que indicará los parámetros de dichas ordenes.

Cada guante tiene en la punta de los dedos unos marcadores de color especial, un color que no se encuentre formando parte del entorno (colores muy llamativos con una textura que no generé brillos o sombras). 

\bigskip 
Se determinaron una conjunto de ordenes, las justas para que un objeto pueda describir cualquier trayectoria sobre una superficie plana. Concluimos que estas podrían ser únicamente: avanzar y girar. Es necesario decirle la distancia que tiene que avanzar en cada momento, pero como eso no era simple, se introdujo la orden parar, así mientras se mueve el robot tu decides cuando ha recorrido la distancia oportuna y detenerlo con una orden. También se distinguió en la orden girar, entre girar a la izquierda y girar a la derecha. Con esto tenemos 4 tipos de ordenes distintas para dar al robot. Pero aun el robot necesita mas información sobre estas ordenes, como por ejemplo en la orden de giro, con cuantos grados tiene que realizarlo o en la orden de avanzar a cuanta velocidad debe moverse. Siguiendo con el objetivo de la simplicidad en vez de añadir mas gestos diferentes a la misma mano, se utilizo la otra mano, es decir, una mano indicaría las ordenes al robot y otra los parámetros según el tipo de orden. 

Los parámetros son o de velocidad o de grados de giro, la velocidad puede ser nula, medio baja, medio alta o alta y los ángulos de giro pueden ser 0º, 45º, 90º o 180º, es decir, cuatro parámetros en ambos casos, se utilizan los mismos símbolos para velocidad como para giro, por tanto solo existen 4 gestos diferentes que se puedan dar con la mano derecha para expresar los parámetros al robot. 

La coincidencia del numero de gestos utilizados para ordenes y el numero de gestos utilizados para parámetros, simplificara la implementación de la red. Y el hecho de usar los mismos gestos para representar las ordenes con la mano izquierda y los parámetros con la mano derecha, simplificara también el entrenamiento de la red.

\bigskip 
Los gestos elegidos son diferentes posiciones de los marcadores del guante, lo mas claro posible, para que después del filtrado la red no tenga problema para diferenciar unos símbolos de otros.

Los marcadores son las únicas áreas de la imagen que no se eliminaran de la imagen. Estas regiones pasarán a ser blancas y el resto negro. Por tanto repito los gestos tienen que ser los suficientemente distintos unos de otros, para que una vez filtrados, esas zonas blancas puedan diferenciarse a simple vista y saber a que orden se están refiriendo. 

Es en ejecución cuando se decide a través de una ventana proporcionada por el pipeline el color de la imagen a filtrar, así que el color de los marcadores del guante no tienen porque ser fijos, se pueden determinar en cada momento. Eso si, los colores de ambos guantes deben de ser distintos y especificarse que color será el que representa a las ordenes y cual representara a los parámetros.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2,bb=0 0 320 240]{sinFiltrar.png}
  \includegraphics[scale=0.4,bb=0 0 160 120]{_orden_parada_17.png}
  \caption{ La imagen de la izquierda muestra la imagen que le llega al filtro. La de la derecha muestra la imagen una vez filtrada.}
\end{figure}

Las fases de este filtro son:
\begin{itemize}
\item Suavizado. Por promediado del entorno.
\item Extracción de regiones por el color.
\item Centrado de imagen. 
\end{itemize} 

\subsection{Suavizado de la imagen}
Difumina la imagen. El suavizado es una transformación de vecindad, donde el valor del nuevo píxel depende de los valores de los píxeles que le rodean. Nuestro método de filtro es un suavizado por el promediado del entorno de vecindad.
\bigskip 

Se utiliza para la eliminación de ruidos y otros efectos debidos a la cuantización o a perturbaciones. La razón de haber utilizado un suavizado al principio fue para crear un difuminado de la imagen y que futuros filtrados sean mas uniformes. 

Cuando no se aplica, la extracción de regiones posterior no es muy fiable, ya que a causa de la iluminación o de la textura del material utilizado en el color especial, hay zonas del objeto de interés que no tienen el mismo color pudiendo pasar por ejemplo de ser un rojo casi blanco a un rojo casi negro, esta amplitud de color es inadmisible para la extracción de colores, ya que esos píxeles serían considerados fuera de rango y por tanto como elementos del entorno y no como elementos de interés. Esto repercutiría en la red neuronal posterior, la cual tiene que asignar pesos según el valor de los píxeles de entrada, si no podemos determinar unos valores fiables en las imágenes de entrada no se podrá entrenar de forma fiable la red ni poder asegurar un comportamiento seguro en el futuro.

\bigskip 
Las desventajas de este método son que desdibuja contornos y detalles de forma, pero en nuestro caso concreto esto no tiene relevancia.
\begin{center}
\textbf{g(x, y) = $ (\frac{1}{K}) * \Sigma f(n, m) $} 
\end{center} 
(sumatorio de 0 a K, siendo K el numero total de puntos de la vecindad)

Método:
\begin{center}
Se utiliza una máscara de convolución $ 5x5 \rightarrow  wi = 1/25 $
\end{center} 
	 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4,bb=0 0 240 160]{arbol1.png}
  \includegraphics[scale=0.4,bb=0 0 240 160]{arbolSuav.png}
  \caption{ Imagen original e imagen suavizada.}
\end{figure}

\subsection{Extracción de regiones de color}
Como hemos dicho los objetos de interés son las puntas de los dedos, así que tenemos que aislarlas del resto de la fotografía. La forma es delimitar estas regiones y darlas toda la importancia respecto al resto de la imagen.

Queremos que el filtro convierta una imagen capturada por la webcam en una imagen blanca y negra, donde las puntas de los guantes quedaran en blanco y el resto de la imagen en negro. Así la red solamente será entrenada para recibir imágenes con regiones blancas y negras, si hay una sola región de un cierto tamaño implicaría que solo hemos enseñado un dedo del guante, lo que se correspondería con el gesto de avanzar. 

Esto es a lo que llamamos segmentación, ya que estamos localizando los objetos de interés.

\bigskip 
Por tanto nuestro objetivo es binarizar la imagen basándonos en el hecho de que los píxeles de una determinada región presentan una distribución de intensidad similar, por tanto, a partir del histograma de los niveles en los tres colores, determinamos cual es la zona de dicho histograma y por tanto la región de la imagen.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5,bb=0 0 280 210]{regoines_histograma.png}
  \caption{ Ejemplo del histograma de intensidades de una imagen.}
\end{figure}

\bigskip 
Basándonos en el modelo de color RGB, se pueden extraer de la imagen aquellas regiones en las que predomine una determinada componente de color.

El método consiste en elegir un determinado predicado y determinar en toda la imagen los píxeles que cumplen dicho predicado. Esos píxeles los marcamos en blanco y el resto en negro, de esta forma obtenemos una imagen binaria. 

\bigskip
Es difícil determinar cual es el umbral optimo para poder llevar acabo una binarización adecuada. Además debemos tener en cuenta que la iluminación que habrá de unas ocasiones a otras será distinta, esto influye en la manera en que la cámara percibe los colores del entorno, por ejemplo, si hay poca luz los colores serán mas oscuros y lo contrario si hubiera mucha luz, por tanto no hemos podido determinar un umbral fijo porque este será dependiente del entorno.

La interfaz del pipeline genera para cada modulo de filtro una pequeña ventana que permite la elección de un color de las imágenes que están entrando en ese momento por la webCam. De esta manera nos estamos asegurando de seleccionar y fijar el color exacto en esas condiciones del entorno. 

Debido a que las imágenes son a color, al seleccionar un color del entorno, se estarán fijando automáticamente 3 umbrales, uno para el rojo, otro para el verde y otro para el azul.

\bigskip 
Si las tres componentes del píxel (x,y) están dentro del rango seleccionado, entonces las 3 componentes tomaran el valor blanco (255) y si estan fuera de rango tomaran el valor negro (0). Binarizando así la imagen.
\bigskip 

Como elegir la tolerancia de este rango. Cuanto mas amplio sea el rango mas cantidad de colores entraran dentro de este. Aunque elijamos el color exacto del entorno que queramos filtrar, si no fijamos bien la tolerancia del rango, la imagen no binarizará las regiones correctas.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4,bb=0 0 175 175]{regiones_tol_baja.png}
  \includegraphics[scale=0.4,bb=0 0 175 175]{regiones_tol_normal.png}
  \includegraphics[scale=0.4,bb=0 0 175 175]{regiones_tol_alta.png}
  \caption{ Extracción de una región de color de una imagen con tolerancia baja, media y alta.}
\end{figure}

\subsection{Centrado de la imagen}
Los gestos realizados con los guantes nunca son capturados por la webcam en la misma posición. Nunca estarán totalmente centrados, si no ligeramente o totalmente desplazados mas a la derecho o mas a la izquierda, arriba o abajo. Esto es de vital importancia para la red, ya que entrena y reconoce en relación a los valores de los píxeles de la imagen, si aprende que el símbolo de avanzar es una región blanca sobre un fondo negro situada siempre en el centro de la imagen, cuando este en fase de reconocimiento y la webcam capture un gesto desplazado, la red lo considerara como gesto no reconocido.
Otra opción podría ser entrenar la red para que reconociese el mismo gesto en cualquier posición, pero eso no podría nunca servir en el entrenamiento, y que los pesos no terminarían nunca de fijarse, ya que el píxel (x,y) si esta blanco para unas imágenes se considerara como ejemplo de entrenamiento positivo, para otras se considerara negativo y los pesos no podrán ajustarse. Por tanto hay que intentar conseguir que las regiones de interés extraídas estén siempre situadas mas o menos en la misma zona de la imagen. Para eso decidimos que la mejor forma de hacer esto era centrar la imagen según el centro de masas del conjunto de píxeles de interés.

\bigskip
Todos los píxeles cuyo color esté dentro de este rango, formaran un conjunto. El conjunto de las coordenadas cartesianas de estos píxeles dentro de la imagen.

Vamos a llamar centro de masas (c. m.), a la media de las coordenadas de todos los píxeles que forman el conjunto, de esta manera el centro de masas será la coordenada de un píxel que puede o no pertenecer al conjunto, pero que representa el centro de la mayor concentración de elementos de este.
La coordenada x será la media aritmética de todas las coordenadas x de este conjunto y lo mismo con la y.
\bigskip 

$ X = \frac{\Sigma x_{i}}{k} $ , desde i=1..k, siendo k el cardinal del conjunto y $ \forall x_{i} \epsilon conjunto $.

$ Y = \frac{\Sigma y_{i}}{k} $ , desde i=1..k, siendo k el cardinal del conjunto y $ \forall y_{i} \epsilon conjunto $.
\bigskip 

El objetivo es centrar el centro de masas dentro de la imagen. Así estaremos centrando la región de interés. El centrado implica un desplazamiento de todos los píxeles de la imagen.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3,bb=0 0 308 201]{centrado.png}
  \includegraphics[scale=0.3,bb=0 0 308 201]{centrado2.png}
  \includegraphics[scale=0.3,bb=0 0 308 201]{centrado3.png}
  \includegraphics[scale=0.3,bb=0 0 308 201]{centrado4.png}
  \caption{ Imagen en blanco y negro. El color elegido será el negro. La 2º imagen muestra cual sería la coordenada que representa el centro de masas del conjunto de píxeles negros. La 3º cual seria el desplazamiento. Y la 4º el resultado del centrado.}
\end{figure}

Si la posición del c. m. no esta en el centro de la imagen. La distancia que hay que desplazar la imagen es el modulo entre el punto central de la imagen y el punto del centro de masas. Por tanto movemos todos los píxeles de la imagen n posiciones verticalmente y m posiciones horizontalmente para cuadrar el c. m. con el centro de la imagen. En este proceso hay píxeles de la imagen original que se pierden y otros que se crean y no tienen un valor concreto, estos serán creados con el color del entorno, ya que se supone que no son de interés.

\bigskip 
Así siempre las regiones blancas que representan los gestos aparecen centrados en la imagen sobre un fondo negros, totalmente preparados para ser pasados a la entrada de la red neuronal.

\subsubsection{Curiosidad del centrado}
El centrado tiene otra utilidad. Debido a que necesita hacer el calculo de cuantos píxeles cumplen la propiedad de estar dentro del rango de color, si resulta que no hay en toda la imagen ninguno que la cumpla, el método no devuelve la imagen centrada, si no que devuelve NULL. Tal y como esta implementado el pipeline si un modulo saca como estructura de datos un puntero a NULL, las siguientes operaciones que se realizarían sobre esta estructura dejan de hacerse, esto aumenta la velocidad del programa si no hay gestos o carteles frente al robot, ya que disminuye en gran cantidad el numero de instrucciones realizadas. Se podría decir que el centrado es un detector de objetos de interés que mantiene al pipeline en \textit{stand by} mientras que no se detecten objetos frente a la cámara.

\section{Filtro de carteles}
Los carteles son señales al igual que los gestos percibidos visualmente por el robot. Llevan impresos mensajes, ya sean ordenes, operaciones aritméticas o preguntas sobre datos que el robot posea en su base de datos. Este debe ser capaz de desechar toda la información de la imagen excepto este mensaje.

Es aquí donde se nos planteó la duda de como resolver este problema, como enseñar al robot a desechar todos los píxeles de la imagen excepto aquellos píxeles negros que forman parte de la región de las letras que forman el mensaje. Hubo distintas soluciones al problema, pero la que mejor ha funcionado es la de crear unos carteles de un color especial, es decir, de un color que no suela encontrarse en el entorno y el mensaje de este impreso en letras negras.

Por tanto ya existe una característica que diferencia a los píxeles del mensaje del resto, y es que son lo únicos píxeles negros rodeados en todas sus direcciones por píxeles con la intensidad propia del color especial. En resumen este filtro lo que hace es binarizar la imagen, el objetivo perseguido es parecido al del filtro de los guantes, sen intenta binarizar la imagen dejando a un color lo importante y en otro color lo que no nos interesa. La diferencia es que el procesamiento llevado a cabo en este filtro es mas complejo que en el de los guantes, requiere pasar por mas fases de procesamiento.

Así pues este filtro convertirá imágenes que contengan un cartel con un mensaje X en una imagen blanca con el mensaje negro centrado, horizontal y con el menor numero de ruidos, también asegura no filtrar la imagen si no se percibe ningún cartel o si este no esta al 100x100 dentro del campo de visión. 

Todo esto son medidas de seguridad para facilitar el futuro análisis del mensaje por parte del OCR, evitando posibles fallos de este y que sea lo mas fiable posible.
\bigskip 

Estos son ejemplos del filtro. Las imágenes sin cartel no serán procesadas ni pasadas al OCR, para evitar fallos y operaciones innecesarias.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4,bb=0 0 253 203]{cartel.png}
  \includegraphics[scale=0.4,bb=0 0 253 203]{cartel2.png}
  \caption{ 1º Simulación de la captura de un cartel. 2º resultado de filtrar la imagen.}
\end{figure}

Ejemplos de mensajes dentro de un cartel:
\begin{itemize}
\item Ordenes y parámetros:
	Avanzar MediaAlta, Parar, Girar 90, ...
\item Operaciones aritmético-lógicas:
	(150mod15)/5, (true and false), ...
\item Preguntas a su base de datos:
	Nombres creadores, ...
\end{itemize} 	

Las fases de este filtro serían estas:
\bigskip

  if( not BORDES and CENTRAR and ESQUINAS)

     ROTAR

     ESQUINAS

     EXTACCION DE REGIONES

     OPERACIÓN DE CIERRE

  end if;

\subsection{Bordes}
En este proyecto hay un factor que siempre tenemos en contra y es el tiempo. Hay que recordar que la captura, procesamiento y análisis de las imágenes se hacen en tiempo real, si la captura de imágenes se hace cada X milisegundos hay que asegurarse que todas las operaciones que se deban de realizar tardan menos que ese intervalo. Por tanto los filtros no solo tienen el deber de procesar la imagen si no también de mejorar el rendimiento del programa.

Por ello son utilizados como sensores de detección de ciertos objetos, en este caso de carteles. 

Esta fase es una función que recibe una imagen de entrada y detecta si la imagen esta completamente dentro del campo de visión o solamente en parte, si solo ha sido capturado parte del cartel, el mensaje puede que no haya salido entero por tanto carecería de sentido y no seria valido. Si el cartel ha sido capturado en su 100x100 devuelve cierto si no falso. Al devolver falso el resto de operaciones de filtro sobre el cartel se dejan de hacer y se pasa NULL al modulo de OCR para que tampoco realice ninguna operación, mejorando por tanto la eficiencia del programa, ya que solamente realizara operaciones cuando detecte carteles enteros.

La idea para implementar este función es simple, solo hay que buscar píxeles cuya intensidad este dentro del rango de color especial del cartel si existe alguna región de estos píxeles en el borde de la imagen capturada implica que el cartel no ha sido capturado en su totalidad.

\subsection{Centrar}
Esto significa centrar el cartel dentro de la imagen. La imagen quedara desplazada lo necesario para que el cartel se encuentre justo en el medio y por tanto también lo estará el mensaje. Esto no aporta valor añadido al posterior análisis por parte del OCR, sirve para asegurar la integridad del mensaje en las siguientes fases del filtro, además sirve como al igual que en el filtro de los gesto y como la función anterior de BORDES, como un sensor. El anterior detectaba si el cartel estaba entero y este si existe cartel dentro de la imagen.

El centrado en su implementación cuenta el numero de píxeles de color especial, es decir, del cartel. Haciendo una media aritmética de sus posiciones, por esto si se da el caso en que el numero de píxeles de este color detectados es igual a cero implica que no hay cartel y por tanto que las siguientes operaciones que se realicen sobre la imagen no tiene sentido al igual que la ejecución del OCR, por tanto como la función BORDES, CENTRAR devuelve NULL si no hay cartel en la imagen, acelerando la ejecución del programa y si resulta que hay cartel, entonces devuelve la imagen centrada.

El método utilizado es el mismo que en centrado del filtro de gestos.

\subsection{Esquinas}
Esta función hace un barrido de la imagen sabiendo ya que existe un cartel en ella, con el objetivo de encontrar la cuatro esquinas de este. Las esquinas son coordenadas cartesianas, cuyo conocimiento es de gran utilidad para realizar una posible rotación del cartel, para la extracción de regiones y como sensor.

\begin{itemize}
\item Respecto a la rotación:

Con saber la posición de al menos dos esquinas contiguas de las cuatro del cartel podemos conocer cuantos grados esta inclinado el cartel, dentro de la imagen. Ya que dos puntos forman un vector, solo hay que calcular el ángulo que forma este vector con algún eje de coordenadas y sabremos cuanto esta inclinado el cartel, para su posterior rotación.
\item Respecto a la extracción de regiones: 

Conocer las esquinas es conocer los limites de lo que nos interesa y de lo que no, ya sabemos que todo lo que este fuera de las esquinas es desechable y lo que queda dentro es necesario procesarlo. 
\item Respecto al sensor:

Conocer al menos tres esquinas, es conocer tres coordenadas y por tanto eso nos permite crear dos vectores. Un vector es un lateral del cartel y el otro la base, sabiendo esto si el ángulo que existe entre esos vectores es un ángulo recto, significa que lo que se esta detectando es un cartel, si no cumple esta propiedad es que se ha detectado un objeto del color buscado, pero no es un cartel. Esto evita pasarle al OCR posible información sin sentido que podría generar fallos en el programa principal.
\end{itemize} 

\subsection{Rotar}
Como ya hemos dicho el análisis siguiente a este procesamiento es llevado a cabo por el OCR, este es un modulo implementado de tal forma que no es sensible al tamaño y en cierta medida al formato de los caracteres, pero si que es sensible cuando estos se encuentran rotados. Muchas veces el robot detectara y procesara carteles que no están completamente horizontales, lo que provocaría fallos en el OCR, por tanto es necesario rotar la imagen lo necesario para que el cartel quede horizontal. Con la función anterior de las esquinas ya sabemos cuantos grados esta girado el cartel, solo hay que pasarle a esta función como parámetros la imagen y los grados a girar.

\bigskip
Esta transformación implica un cambio en la disposición y distribución de los píxeles respecto de un sistema de coordenadas.

Para estas tres transformaciones elementales, es necesario hacer una interpolación a la imagen resultante. A efectos de notación, la imagen original tendrá coordenadas (i, j) y la imagen resultante coordenadas (x, y). Estas coordenadas están asociadas a un determinado sistema de referencia, que es preciso establecer con su origen y su convenio de ejes.

Vamos ha utilizar coordenadas homogéneas ya que permiten realizar la rotación mediante el uso de matrices.

\bigskip
La transformación de rotación consiste en que para cada píxel de la imagen original se calcula su correspondiente píxel de la imagen destino. Una vez sabemos la posición del píxel le asignamos a este la misma intensidad que el de la original. Este sistema puede producir una imagen destino rotada con muchos píxeles en blanco, es decir, píxeles que no se asocian con ninguno de la original siguiendo la matriz de transformación vista.
La única forma de asignar un valor de intensidad a todos los píxeles de la imagen destino, es hacer la transformación inversa, donde en vez de hallar el píxel destino (x, y) a partir de un píxel (i, j), hallamos el píxel (i, j) a partir del (x, y). Ahora podemos recorrer toda la imagen destino viendo cual es el píxel origen asociado a él y asignarle así su intensidad, sin que queden píxeles sin color asignado. Ahora puede que varios píxeles destino compartan el mismo píxel origen, pero eso es menos grave que ver una imagen rotada con casi un tercio de espacios en blanco.

\bigskip
La rotación en sentido opuesto viene dada por la siguiente transformación:

$ i = x.cos\theta - y.sen\theta $

$ j = -x.sen\theta + y.cos\theta $

(i, j) es un píxel de la imagen origen y (x, y) uno de la imagen destino.

\bigskip
Los píxeles de la imagen resultante que tras el calculo se les asigna un píxel de la imagen original que se sale de rango, se le asigna una intensidad correspondiente con la del entorno. En el ejemplo anterior se les asigno el color negro.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4,bb=0 0 250 188]{no_rotada.png}  
  \includegraphics[scale=0.3,bb=0 0 291 247]{rotada.png} 
  \caption{ Ejemplo del funcionamiento del algoritmo de rotación.}
\end{figure}

\subsection{Extracción regiones}
Esta parte es similar a la utilizada en el filtro de gestos. Solo que aquí no nos interesan las regiones de color especial, si no ciertas regiones rodeadas de este color especial, que en este caso serán los caracteres del mensaje.

La extracción de regiones binariza la imagen convirtiendo a un color la región buscada y a otro el resto de la imagen, las regiones del mensaje pasaran a negro y el resto a blanco, como en el ejemplo anterior. 

La función ESQUINAS se realizara otra vez después de la rotación (si esta ha sido necesaria) ya que la posición de estas dentro de la imagen habrá cambiado. Como ya hemos dicho antes, saber las coordenadas de estas esquinas nos informa de que área de la imagen es necesario filtrar y cual directamente es considerada como región no extraíble y puesta directamente a color blanco, en este caso será toda aquella región que quede fuera del cartel.

La extracción de regiones propiamente dicha se realizara por tanto dentro del área de la imagen perteneciente al interior del cartel. Las regiones que se extraerán serán aquellos píxeles que cumplan la propiedad de ser oscuros rodeados en todas sus direcciones por color especial. 

\subsection{Operacion morfológica}
Después de todas las operaciones anteriores realizadas sobre la imagen los caracteres del cartel puede que hayan quedado dañados, es decir, como si se hubieran erosionado, perdiendo suavidad en sus bordes o incluso dejar un mismo carácter separado en regiones distintas.

Nuestro cerebro tiene la necesidad de encontrar sentido a lo que ve y a unificar figuras inacabadas, pero esta cualidad no la tiene el OCR, si este detecta regiones separadas las tratara como distintos caracteres y buscara el carácter mas aproximado a estas, lo cual sería un error. Es necesario por tanto juntar regiones que hayan quedado separadas y rellenar huecos de los caracteres del mensaje para asegurar por tanto una mejor comprensión del mensaje.

La operación morfológica realizada sobre la imagen es una operación de cierre para rellenar los caracteres.

\bigskip
Las operaciones morfológicas simplifican las imágenes y preservan las formas principales de los objetos. La morfología puede utilizarse para suavizar bordes de una región, separar regiones que el proceso de segmentación presenta unidas o como en nuestro caso unir regiones que fueron separadas durante la segmentación.

Una transformación morfológica $ \theta $ viene dada por una relación de la imagen (conjunto de puntos X) con otro pequeño conjunto de puntos B, llamado elemento estructural. B se expresa con respecto a un origen local O (llamado punto representativo o punto de vista director). 

La transformación morfológica $ \theta(X) $ aplicada a la imagen X significa que el elemento estructural B se desplaza por toda la imagen. Suponiendo que B se posiciona sobre algún punto de la imagen, el píxel de la imagen correspondiente al punto representativo O de B se denomina píxel actual. El resultado de la relación entre la imagen X y el elemento estructural B en la posición actual se almacena en el píxel actual de la imagen.

\subsubsection{Dilatación}
La dilatación es una operación isotrópica ya que se comporta igual en todas las direcciones y expansiona el objeto un píxel. Esta operación en ocasiones se denomina rellenado o crecimiento.
La dilatación con un elemento estructural 3x3 isótropo puede interpretarse como una dilatación que cambia todos los píxeles de fondo que son vecinos al objeto.
Gráficamente la dilatación se realiza como sigue: se va recorriendo la imagen por ejemplo de izquierda a derecha y de arriba abajo y, donde nos encontremos un 1, situamos el origen del elemento estructural; en esa posición se realiza la unión del elemento estructural con la parte de la imagen sobre la que se solapa dicho elemento, marcando todos los píxeles de esta parte de la imagen con los valores del elemento B.

\subsubsection{Erosión}
La erosión re realiza de forma parecida a la dilatación solo que cuando recorriendo la imagen original, nos encontramos un píxel con valor 1, aplicamos el elemento estructural no para asignarle sus valores  a la imagen destino, si no como comparación. Si todos los píxeles del elemento B coinciden con los valores de los píxeles de la región de la imagen entonces ponemos el valor de píxel de la imagen destino con valor 1, si no con valor 0.
Se puede apreciar la desaparición de muchos contornos existentes en la imagen original. La erosión es denominada como reducción, utilizada para simplificar la estructura de los objetos, ya que los objetos con anchos pequeños desaparecen , por tanto, objetos complicados pueden descomponerse en otros mas simples.

\subsubsection{Apertura y Cierre}
LA erosión y la dilatación son transformaciones no invertibles. Si una imagen es erosionada y luego dilatada, la imagen original no se recupera. En efecto, el resultado es una imagen mas simplificada y menos detallada que la original.
La erosión seguida de una dilatación crea una transformación morfológica denominada apertura. La dilatación seguida de una erosión crea una transformación llamada cierre.
La apertura y el cierre con un elemento estructural isótropo se utiliza para eliminar detalles específicos de la imagen mas pequeños que el elemento estructural. La forma global de los objetos no se distorsiona.
El cierre conecta objetos que están próximos entre sí, y rellena pequeños huecos y suaviza el contorno del objeto rellenando pequeños valles, mientras que la apertura produce el efecto contrario. Los conceptos de pequeño y próximo están relacionados con la forma del elemento estructural.

\begin{figure}
  \centering
  \includegraphics[scale=0.4,bb=0 0 275 215]{original.png}
  \includegraphics[scale=0.4,bb=0 0 275 215]{binarizada.png}
  \includegraphics[scale=0.4,bb=0 0 275 215]{cierre.png}
  \caption{ Original. Binarizada segun un umbral. Y ejemplo de cierre sobre esa imagen.}
\end{figure}

\section{Código}
Ver anexo: documentación del módulo de filtro\_gestos.c, en \ref {filtro_code} (página \pageref{filtro_code}).

